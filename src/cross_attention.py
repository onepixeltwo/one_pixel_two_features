# -*- coding: utf-8 -*-
"""cross attention modification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UKy8_gTRIP1FSwuXweW1uxzIn6kltx7z

Modification on June 18
1.  Adding multihead
2.  cross attention using cls token
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pdb
# token 244 bands into 21tokens

class SpectralTokenizer(nn.Module):
    def __init__(self,in_channels=1, out_channels=64, kernel_size=12, stride=12, padding=0):
        super(SpectralTokenizer, self).__init__()
        self.conv1d = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,
                                kernel_size=kernel_size, stride=stride, padding=padding)

    def forward(self, x):
        # x should be of shape (batch_size, 1, 244)
        x = self.conv1d(x)  # Apply convolution
        return x

# build cross attention class using builit in multihead June/17
# embedding dimension = 256
# num_head = 4


class CrossAttention(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads = 4):
        super(CrossAttention, self).__init__()


        # Initialize the multi-head attention layer
        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)


        # Linear transformations for query, key, and value
        self.query_transform = nn.Linear(input_dim, embed_dim)
        self.key_value_transform = nn.Linear(input_dim, embed_dim)

        # Output transformation
        self.out_transform = nn.Linear(embed_dim, input_dim)

        # Layer normalization
        self.norm = nn.LayerNorm(input_dim)


    def forward(self, cls, key_value):
     # cls shape [batch_size, seq_len, ]

        # Transform query, key, and value
        query = self.query_transform(cls).permute(1, 0, 2)  # [seq_len, batch_size, hidden_dim]
        key_value = self.key_value_transform(key_value).permute(1, 0, 2)  # [seq_len, batch_size, hidden_dim]

        # Apply multi-head attention
        attn_output, _ = self.multihead_attn(query, key_value, key_value)

        # Apply the output transformation
        output = self.out_transform(attn_output.permute(1, 0, 2))  # Convert back to [batch_size, seq_len, hidden_dim]

        # Add residual connection and normalize
        output_with_residual = self.norm(output + cls)



        return output_with_residual

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads= 4 , dropout= 0.1, forward_expansion = 1):
        super(SelfAttention, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)

        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self,x): # input x (batch_size, seq_len, feature_dim)

        query = x.permute(1, 0, 2)  #(seq_len, batch_size ,feature_dim )
        attention_output, _ = self.attention(query, query, query)
        x = self.dropout(self.norm1(attention_output.permute(1,0,2) + x))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out

class DualPathNetwork_cross(nn.Module):
    # cross attention modality fusion ,neweset version for Berlin dataset June/18
    def __init__(self, num_class, embed_dim =256, spatial_dim = 8064):
        super(DualPathNetwork_cross, self).__init__()


        self.spectral_token_dim = 64
        self.spatial_token_dim = 512

        # project spatial token to self.spatial_token_dim
        self.spatial_process =  nn.Linear(spatial_dim//3 , self.spatial_token_dim)


        self.spectral_tokenizer = SpectralTokenizer()

        self.cls_token_spectral = nn.Parameter(torch.randn(1, 1, self.spectral_token_dim))
        self.cls_token_spatial  = nn.Parameter(torch.randn(1, 1, self.spatial_token_dim))



        #self.pos_embedding_spectral = nn.Parameter(torch.randn(36 + 1, self.out_channels)*0.01) # 36 if using UH dataset
        #self.pos_embedding_spectral = nn.Parameter(torch.randn(21 + 1, self.spectral_token_dim)*0.01) # if using berlin dataset
        self.pos_embedding_spectral = nn.Parameter(torch.randn(15 + 1, self.spectral_token_dim)*0.01) # if using augsburg dataset
        self.pos_embedding_spatial = nn.Parameter(torch.randn(3 + 1, self.spatial_token_dim)*0.01)


        self.proj_spatial_dim = nn.Linear(self.spectral_token_dim, self.spatial_token_dim)
        self.proj_spectral_dim = nn.Linear(self.spatial_token_dim, self.spectral_token_dim)


        self.spectral_self_attn =  SelfAttention(self.spectral_token_dim, heads = 4)
        self.spatial_self_attn = SelfAttention(self.spatial_token_dim, heads = 4)

        self.spatial_cross_spectral = CrossAttention(self.spectral_token_dim, embed_dim)
        self.spectral_cross_spatial = CrossAttention(self.spatial_token_dim, embed_dim)

        # Trainable weights for combining outputs
        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32))
        self.classifier_spatial = nn.Linear(self.spatial_token_dim, num_class)
        self.classifier_spectral = nn.Linear(self.spectral_token_dim, num_class)

    def forward(self, X_spatial, X_spectral):


        batch_size = X_spectral.size(0)

        # tokenize spectral data
        X_spectral =X_spectral.unsqueeze(1) # (batch_size, 1, 244)
        X_spectral = self.spectral_tokenizer(X_spectral)   # (batch_size ,out_channels, seq_length)
        X_spectral = X_spectral.transpose(1,2)  #(batch_size, seq_length, out_channels)

        # tokenize X_spatial
        X_spatial = X_spatial.view(batch_size, 3, X_spatial.shape[1]//3 ) #(batch_size,3, out_channels)  # tokenize X_spatial(batch_size, 3 ,out_v)
        X_spatial = self.spatial_process(X_spatial) #(batch_size,  out_channels ) (batch_size, seg_len, outchannels)


        #X_spatial = X_spatial. #(batch_size,3, out_channels)  # tokenize X_spatial (batch_size, 3 ,out_v)
        # cls token for spectral , X_spatial is aslo cls_spatial itself
        cls_spectral = self.cls_token_spectral.expand(batch_size,-1, -1)
        cls_spatial = self.cls_token_spatial.expand(batch_size,-1,-1)

        # X_spectral_all  = cls_spectral || X_spectral
        X_spectral_all = torch.cat((cls_spectral, X_spectral), dim=1) + self.pos_embedding_spectral.unsqueeze(0)
        X_spatial_all =  torch.cat((cls_spatial, X_spatial), dim=1) + self.pos_embedding_spatial.unsqueeze(0)

        # spectral embedding has
        spectral_self_attn_times = 1
        for i in range(0, spectral_self_attn_times):
          X_spectal_all = self.spectral_self_attn(X_spectral_all)

        spatial_self_attn_times = 2
        for self_spatial_attention_times in range(0,spatial_self_attn_times):
          X_spatial_all = self.spatial_self_attn(X_spatial_all)


        cls_spectral= X_spectral_all[:,0,:].unsqueeze(1)
        cls_spatial = X_spatial_all[:,0,:].unsqueeze(1)

        cross_attn_times = 2
        for i  in range(0, cross_attn_times):

          cls_spectral = self.proj_spatial_dim(cls_spectral)
          cls_spatial = self.proj_spectral_dim(cls_spatial)

          # for cross attention, if cls_spatial is query, then key and value is projection(cls_spatial)|| X_spectral
          concate_spatial_cls_spectral = torch.cat((cls_spatial,X_spectral),dim =1)  #(batch_size, seg_len, out_channels)
          # cls_spectral is query, the key and value  is projection(cls_spectral) || X_spatial
          concate_spectral_cls_spatial =  torch.cat((cls_spectral,X_spatial),dim =1)

          # model specific cross attention
          spatial_attended = self.spatial_cross_spectral(cls_spatial, concate_spatial_cls_spectral ) # output is spectral token feature dim , project back to spatial_dim
          # poject back to spatial dimention
          cls_spatial =self.proj_spatial_dim(spatial_attended)

          spectral_attended = self.spectral_cross_spatial(cls_spectral, concate_spectral_cls_spatial)
          cls_spectral = self.proj_spectral_dim(spectral_attended)

        # Normalize and combine weights
        weights = F.softmax(self.weights, dim=0)
        spatial_logits = self.classifier_spatial(cls_spatial.squeeze(1))
        spectral_logits = self.classifier_spectral(cls_spectral.squeeze(1))
        logits  = weights[0] * spatial_logits + weights[1] * spectral_logits

        return logits

